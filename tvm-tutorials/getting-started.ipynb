{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global declarations of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_host=\"llvm\"\n",
    "# Change it to respective GPU if gpu is enabled Ex: cuda, opencl\n",
    "tgt=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tvm.var(\"n\")\n",
    "A = tvm.placeholder((n,), name='A')\n",
    "B = tvm.placeholder((n,), name='B')\n",
    "C = tvm.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.tensor.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule the Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A schedule is a set of transformation of computation that transforms the loop of computations in the program\n",
    "\n",
    "- After we construct the schedule, by default the schedule computes C in a serial manner in a row-major order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tvm.create_schedule(C.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.schedule.Schedule"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Schedule\n",
       "\u001b[0;31mString form:\u001b[0m schedule(0x1d46a90)\n",
       "\u001b[0;31mFile:\u001b[0m        ~/opt/miniconda3/envs/tvm/lib/python3.6/site-packages/tvm-0.2.0-py3.6-linux-x86_64.egg/tvm/schedule.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   Schedule for all the stages.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the split construct to split the first axis of C\n",
    "    - this will split the original iteration axis into product of two iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx, tx = s[C].split(C.op.axis[0], factor=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter_var(i.outer, )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter_var(i.inner, )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally bind the iteration axis `bx` and `tx` to threads in the GPU compute grid. \n",
    "- These are GPU specific constructs that allows us to generate code that runs on GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt == \"cuda\":\n",
    "    s[C].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
    "    s[C].bind(tx, tvm.thread_axis(\"threadIdx.x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After finishing to specify the schedule, we can compile it into a TVM function. \n",
    "- By default TVM compiles into a type-erased function that can be directly called from python side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd = tvm.build(sch=s, args=[A, B, C], target=tgt, target_host=tgt_host, name=\"myadd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(llvm, 21b49b0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.module.Module"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m   \u001b[0mfadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m        Module\n",
       "\u001b[0;31mString form:\u001b[0m Module(llvm, 21b49b0)\n",
       "\u001b[0;31mFile:\u001b[0m        ~/opt/miniconda3/envs/tvm/lib/python3.6/site-packages/tvm-0.2.0-py3.6-linux-x86_64.egg/tvm/module.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   Module container of all TVM generated functions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fadd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a gpu context\n",
    "- Use `tvm.nd.array` to copy data to gpu\n",
    "- fadd runs the actual computation\n",
    "- Use `asnumpy()` to copy the gpu array to cpu so that we can use this to verify correctness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = tvm.context(dev_type=tgt, dev_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx=ctx)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx=ctx)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 ms, sys: 274 Âµs, total: 17.2 ms\n",
      "Wall time: 17.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fadd(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0944474 , 1.3696101 , 1.4258708 , ..., 1.0076509 , 1.4810429 ,\n",
       "       0.42075178], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6563842 , 0.5160306 , 0.96769214, ..., 0.21516554, 0.57610613,\n",
       "        0.11985361], dtype=float32),\n",
       " array([0.43806314, 0.8535795 , 0.45817864, ..., 0.7924853 , 0.9049368 ,\n",
       "        0.30089816], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.asnumpy(), b.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.NDArray shape=(1024,), gpu(0)>\n",
       "array([0.6563842 , 0.5160306 , 0.96769214, ..., 0.21516554, 0.57610613,\n",
       "       0.11985361], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Generated Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------GPU code--------------------------\n",
      "extern \"C\" __global__ void myadd__kernel0( float* __restrict__ C,  float* __restrict__ A,  float* __restrict__ B, int n) {\n",
      "  if (((int)blockIdx.x) < (n / 64)) {\n",
      "    C[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (A[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + B[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n",
      "  } else {\n",
      "    if ((((int)blockIdx.x) * 64) < (n - ((int)threadIdx.x))) {\n",
      "      C[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (A[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + B[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if tgt == \"cuda\":\n",
    "    dev_module = fadd.imported_modules[0]\n",
    "    print(\"-----------------------GPU code--------------------------\")\n",
    "    print(dev_module.get_source())\n",
    "    \n",
    "else:\n",
    "    print(fadd.get_source())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Compiled Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Besides runtime compilation, we can save the compiled modules into file and load them back later. This is called ahead of time compilation\n",
    "\n",
    "- The following code first does the following step:\n",
    "    - It saves the compiled host module into an object file.\n",
    "    - Then it saves the device module into a ptx file.\n",
    "    - cc.create_shared calls a env compiler(GCC) to create a shared library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.contrib import cc\n",
    "from tvm.contrib import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = util.tempdir()\n",
    "fadd.save(file_name=temp.relpath(\"myadd.o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt == \"cuda\":\n",
    "    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.create_shared(output=temp.relpath(\"myadd.so\"), objects=[temp.relpath(\"myadd.o\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myadd.tvm_meta.json', 'myadd.o', 'myadd.ptx', 'myadd.so']\n"
     ]
    }
   ],
   "source": [
    "print(temp.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Compiled Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd1 = tvm.module.load(temp.relpath(\"myadd.so\"))\n",
    "if tgt == \"cuda\":\n",
    "    fadd1_dev = tvm.module.load(temp.relpath(\"myadd.ptx\"))\n",
    "    fadd1.import_module(fadd1_dev)\n",
    "fadd1(a, b, c)\n",
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack Everything into One Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we store the device and host code seperatedly. TVM also supports export everything as one shared library. Under the hood, we pack the device modules into binary blobs and link them together with the host code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd.export_library(temp.relpath(\"mypack.so\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd2 = tvm.module.load(temp.relpath(\"mypack.so\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd2(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Runtime API and Thread-Safety\n",
    "\n",
    "The compiled modules of TVM do not depend on the TVM compiler. Instead, it only depends on a minimum runtime library. TVM runtime library wraps the device drivers and provides thread-safe and device agnostic call into the compiled functions.\n",
    "\n",
    "This means you can call the compiled TVM function from any thread, on any GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version_information extension is already loaded. To reload it, use:\n",
      "  %reload_ext version_information\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.5 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]"
        },
        {
         "module": "IPython",
         "version": "6.4.0"
        },
        {
         "module": "OS",
         "version": "Linux 4.13.0 41 generic x86_64 with debian stretch sid"
        },
        {
         "module": "numpy",
         "version": "1.14.3"
        },
        {
         "module": "tvm",
         "version": "0.2.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.5 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]</td></tr><tr><td>IPython</td><td>6.4.0</td></tr><tr><td>OS</td><td>Linux 4.13.0 41 generic x86_64 with debian stretch sid</td></tr><tr><td>numpy</td><td>1.14.3</td></tr><tr><td>tvm</td><td>0.2.0</td></tr><tr><td colspan='2'>Thu May 17 17:10:38 2018 CDT</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.5 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \\\\ \\hline\n",
       "IPython & 6.4.0 \\\\ \\hline\n",
       "OS & Linux 4.13.0 41 generic x86\\_64 with debian stretch sid \\\\ \\hline\n",
       "numpy & 1.14.3 \\\\ \\hline\n",
       "tvm & 0.2.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Thu May 17 17:10:38 2018 CDT} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.5 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
       "IPython 6.4.0\n",
       "OS Linux 4.13.0 41 generic x86_64 with debian stretch sid\n",
       "numpy 1.14.3\n",
       "tvm 0.2.0\n",
       "Thu May 17 17:10:38 2018 CDT"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
