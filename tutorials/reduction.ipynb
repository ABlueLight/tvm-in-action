{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Sum of Rows\n",
    "\n",
    "`B = numpy.sum(A, axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tvm.var(\"n\")\n",
    "m = tvm.var(\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tvm.placeholder((n, m), name=\"A\")\n",
    "k = tvm.reduce_axis(dom=(0, m), name=\"k\")\n",
    "B = tvm.compute(shape=(n,), fcompute=lambda i: tvm.sum(A[i, k], axis=k), name=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule the Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce B {\n",
      "  for (i, 0, n) {\n",
      "    B[i] = 0.000000f\n",
      "    for (k, 0, m) {\n",
      "      B[i] = (B[i] + A[((i*m) + k)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = tvm.create_schedule(B.op)\n",
    "print(tvm.lower(sch=s, args=[A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split both the row axis of B as well axis by different factors. The result is a nested reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko, ki = s[B].split(parent=B.op.reduce_axis[0], factor=16)\n",
    "xo, xi = s[B].split(parent=B.op.axis[0], factor=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce B {\n",
      "  for (i.outer, 0, ((n + 31)/32)) {\n",
      "    for (i.inner, 0, 32) {\n",
      "      if (likely(((i.outer*32) < (n - i.inner)))) {\n",
      "        B[((i.outer*32) + i.inner)] = 0.000000f\n",
      "      }\n",
      "      for (k.outer, 0, ((m + 15)/16)) {\n",
      "        for (k.inner, 0, 16) {\n",
      "          if (likely(((i.outer*32) < (n - i.inner)))) {\n",
      "            if (likely(((k.outer*16) < (m - k.inner)))) {\n",
      "              B[((i.outer*32) + i.inner)] = (B[((i.outer*32) + i.inner)] + A[(((((i.outer*32) + i.inner)*m) + (k.outer*16)) + k.inner)])\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(sch=s, args=[A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are building a GPU kernel, we can bind the rows of B to GPU threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce B {\n",
      "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = ((n + 31)/32)\n",
      "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 32\n",
      "  if (likely(((blockIdx.x*32) < (n - threadIdx.x)))) {\n",
      "    B[((blockIdx.x*32) + threadIdx.x)] = 0.000000f\n",
      "  }\n",
      "  for (k.outer, 0, ((m + 15)/16)) {\n",
      "    for (k.inner, 0, 16) {\n",
      "      if (likely(((blockIdx.x*32) < (n - threadIdx.x)))) {\n",
      "        if (likely(((k.outer*16) < (m - k.inner)))) {\n",
      "          B[((blockIdx.x*32) + threadIdx.x)] = (B[((blockIdx.x*32) + threadIdx.x)] + A[(((((blockIdx.x*32) + threadIdx.x)*m) + (k.outer*16)) + k.inner)])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s[B].bind(ivar=xo, thread_ivar=tvm.thread_axis(\"blockIdx.x\"))\n",
    "s[B].bind(ivar=xi, thread_ivar=tvm.thread_axis(\"threadIdx.x\"))\n",
    "print(tvm.lower(sch=s, args=[A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction Factoring and Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tvm.create_schedule(ops=B.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko, ki = s[B].split(parent=B.op.reduce_axis[0], factor=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BF = s.rfactor(tensor=B, axis=ki, factor_axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [B.rf] storage_scope = \"global\"\n",
      "allocate B.rf[float32 * 16 * n]\n",
      "produce B.rf {\n",
      "  for (k.inner, 0, 16) {\n",
      "    for (i, 0, n) {\n",
      "      B.rf[((k.inner*n) + i)] = 0.000000f\n",
      "      for (k.outer, 0, ((m + 15)/16)) {\n",
      "        if ((k.inner < (m - (k.outer*16)))) {\n",
      "          B.rf[((k.inner*n) + i)] = (B.rf[((k.inner*n) + i)] + A[((k.inner + (i*m)) + (k.outer*16))])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce B {\n",
      "  for (ax0, 0, n) {\n",
      "    B[ax0] = 0.000000f\n",
      "    for (k.inner.v, 0, 16) {\n",
      "      B[ax0] = (B[ax0] + B.rf[(ax0 + (k.inner.v*n))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(sch=s, args=[A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0.000000f]), source=[B.rf(k.inner.v, ax0)], axis=[iter_var(k.inner.v, Range(min=0, extent=16))], where=(uint1)1, value_index=0)]\n"
     ]
    }
   ],
   "source": [
    "print(s[B].op.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Thread Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now parallelize over the factored axis. \n",
    "- Here the reduction axis of B is marked to be a thread.\n",
    "- TVM allows reduction axis to be marked as thread if it is the only axis in reduction and cross thread reduction is possible in the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extern \"C\" __global__ void default_function__kernel0( float* __restrict__ A,  float* __restrict__ B, int m, int n) {\n",
      "   float B_rf[1];\n",
      "  __shared__ float red_buf0[512];\n",
      "  B_rf[0] = 0.000000e+00f;\n",
      "  for (int k_outer = 0; k_outer < ((15 + m) / 16); ++k_outer) {\n",
      "    if ((((int)blockIdx.x) * 32) < (n - ((int)threadIdx.y))) {\n",
      "      if (((int)threadIdx.x) < (m - (k_outer * 16))) {\n",
      "        B_rf[0] = (B_rf[0] + A[(((((((int)blockIdx.x) * 32) + ((int)threadIdx.y)) * m) + ((int)threadIdx.x)) + (k_outer * 16))]);\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  ((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] = (((((int)blockIdx.x) * 32) < (n - ((int)threadIdx.y))) ? B_rf[0] : 0.000000e+00f);\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) < 8) {\n",
      "    ((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] = (((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] + ((volatile __shared__ float*)red_buf0)[((8 + (((int)threadIdx.y) * 16)) + ((int)threadIdx.x))]);\n",
      "    ((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] = (((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] + ((volatile __shared__ float*)red_buf0)[((4 + (((int)threadIdx.y) * 16)) + ((int)threadIdx.x))]);\n",
      "    ((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] = (((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] + ((volatile __shared__ float*)red_buf0)[((2 + (((int)threadIdx.y) * 16)) + ((int)threadIdx.x))]);\n",
      "    ((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] = (((volatile __shared__ float*)red_buf0)[((((int)threadIdx.y) * 16) + ((int)threadIdx.x))] + ((volatile __shared__ float*)red_buf0)[((1 + (((int)threadIdx.y) * 16)) + ((int)threadIdx.x))]);\n",
      "  }\n",
      "  __syncthreads();\n",
      "  if ((((int)blockIdx.x) * 32) < (n - ((int)threadIdx.y))) {\n",
      "    if (((int)threadIdx.x) == 0) {\n",
      "      B[((((int)blockIdx.x) * 32) + ((int)threadIdx.y))] = ((volatile __shared__ float*)red_buf0)[(((int)threadIdx.y) * 16)];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xo, xi = s[B].split(s[B].op.axis[0], factor=32)\n",
    "s[B].bind(xo, tvm.thread_axis(\"blockIdx.x\"))\n",
    "s[B].bind(xi, tvm.thread_axis(\"threadIdx.y\"))\n",
    "tx = tvm.thread_axis(\"threadIdx.x\")\n",
    "s[B].bind(s[B].op.reduce_axis[0], tx)\n",
    "s[BF].compute_at(s[B], s[B].op.reduce_axis[0])\n",
    "s[B].set_store_predicate(tx.var.equal(0))\n",
    "fcuda = tvm.build(s, [A, B], \"cuda\")\n",
    "print(fcuda.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 128\n",
    "ctx  = tvm.gpu(0)\n",
    "a = tvm.nd.array(np.random.uniform(size=(nn, nn)).astype(A.dtype), ctx)\n",
    "b = tvm.nd.array(np.zeros(nn, dtype=B.dtype), ctx)\n",
    "fcuda(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(b.asnumpy(), np.sum(a.asnumpy(), axis=1), rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Convolution via 2D Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tvm.var(\"n\")\n",
    "Input = tvm.placeholder(shape=(n, n), name=\"Input\")\n",
    "Filter = tvm.placeholder(shape=(3, 3), name=\"Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = tvm.reduce_axis(dom=(0, 3), name=\"di\")\n",
    "dj = tvm.reduce_axis(dom=(0, 3), name=\"dj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = tvm.compute(shape=(n-2, n-2),\n",
    "                    fcompute=lambda i, j: tvm.sum(Input[i + di, j + dj] * Filter[di, dj], axis=[di, dj]),\n",
    "                    name=\"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tvm.create_schedule(ops=Output.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce Output {\n",
      "  for (i, 0, (n + -2)) {\n",
      "    for (j, 0, (n + -2)) {\n",
      "      Output[((i*(n + -2)) + j)] = 0.000000f\n",
      "      for (di, 0, 3) {\n",
      "        for (dj, 0, 3) {\n",
      "          Output[((i*(n + -2)) + j)] = (Output[((i*(n + -2)) + j)] + (Input[((j + ((i + di)*n)) + dj)]*Filter[((di*3) + dj)]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(sch=s, args=[Input, Filter, Output], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define General Commutative Reduction Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tvm.var('n')\n",
    "m = tvm.var('m')\n",
    "product = tvm.comm_reducer(lambda x, y: x*y,\n",
    "    lambda t: tvm.const(1, dtype=t), name=\"product\")\n",
    "A = tvm.placeholder((n, m), name='A')\n",
    "k = tvm.reduce_axis((0, m), name='k')\n",
    "B = tvm.compute((n,), lambda i: product(A[i, k], axis=k), name='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tvm.create_schedule(ops=B.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce B {\n",
      "  for (i, 0, n) {\n",
      "    B[i] = 1.000000f\n",
      "    for (k, 0, m) {\n",
      "      B[i] = (B[i]*A[((i*m) + k)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(sch=s, args=[A, B], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
